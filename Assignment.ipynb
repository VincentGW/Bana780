{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 26: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m     94\u001b[39m merged = pd.DataFrame({\n\u001b[32m     95\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mYamaha\u001b[39m\u001b[33m'\u001b[39m: A_df[\u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     96\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mGibson\u001b[39m\u001b[33m'\u001b[39m: B_df[\u001b[33m'\u001b[39m\u001b[33mDescription\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     97\u001b[39m })\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m#print(merged['Yamaha'][2])\u001b[39;00m\n\u001b[32m    100\u001b[39m \n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m#wv = api.load(\"word2vec-google-news-300\")\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m#vec_king = wv['king']\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m#print(vec_king)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m yamaha = \u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mYamaha\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m gibson = \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(merged[\u001b[33m'\u001b[39m\u001b[33mGibson\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(yamaha)\n",
      "\u001b[31mTypeError\u001b[39m: sequence item 26: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this problem, you will capture data that includes unstructured text and you will serve as a consultant analyzing this unstructured data using Natural Language Processing tools.\n",
    "There are many choices as to how you may do this. \n",
    "\n",
    "Minimum specifications:\n",
    "Identify a company to serve in this assignment. Their data should be part of the dataset you capture. Either...\n",
    "Find an API for download, or... \n",
    "\n",
    "A website where you can scrape a significant number of records (say 500 scraping, 1000 API) of text and other data about your company/product/service.\n",
    "Join at least two different datasets together (these could be multiple pages of web scraping or separate API requests for a product versus its competitor\n",
    "\n",
    "Create an analysis of unstructured data for your company/product and competitors using Python:\n",
    "\n",
    "Help us to understand what is being said about your company/product in the unstructured data (I will demo both API and screen scraping).\n",
    "Use natural language processing transformers to better access the MEANING of what the users are saying (ie. Word2Vec or Doc2Vec models).\n",
    "Create visuals and write explanations in a professional report that shows your findings.\n",
    "'''\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "#import statsmodels.api as sm\n",
    "#from googleapiclient.discovery import build\n",
    "#from gensim.models.word2vec import Word2Vec\n",
    "#from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import nltk\n",
    "\n",
    "# Manage workspaces via Git\n",
    "current_directory = os.getcwd()\n",
    "directory = \"\"\n",
    "if current_directory[9:14] == 'vgwis':\n",
    "    directory = \"C:/Users/vgwis/Documents/Bana780/\"\n",
    "else:\n",
    "    directory = \"C:/Users/vgwcto/Documents/Python/Bana780/\"\n",
    "\n",
    "# Store API key locally\n",
    "api_key = pd.read_csv(directory + 'key.csv').columns[0]\n",
    "\n",
    "# Build a Function to easily search new ideas\n",
    "def multiple_page_search(api_key, query, max_results = 50, max_pages = 10, cache_file = None):\n",
    "    \n",
    "    # Check if cache exists\n",
    "    if cache_file and os.path.exists(cache_file):\n",
    "        return pd.read_csv(cache_file)\n",
    "\n",
    "    # Initialize the YouTube API client, variables\n",
    "    youtube = build('youtube', 'v3', developerKey = api_key)\n",
    "    descriptions = []\n",
    "    next_page_token = None\n",
    "    page_count = 0\n",
    "\n",
    "    # cONSTRUCT the API request\n",
    "    while page_count < max_pages:\n",
    "        search = youtube.search().list(\n",
    "            q = query,\n",
    "            part = 'snippet',\n",
    "            type = 'video',\n",
    "            maxResults = max_results,\n",
    "            pageToken = next_page_token\n",
    "        )\n",
    "        response = search.execute()\n",
    "\n",
    "        # Collect video IDs from the response to retrieve full descriptions\n",
    "        video_ids = [item['id']['videoId'] for item in response['items']]\n",
    "\n",
    "        video_request = youtube.videos().list(\n",
    "            part = 'snippet',\n",
    "            id = ','.join(video_ids)\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "\n",
    "        for item in video_response['items']:\n",
    "            desc = item['snippet']['description']\n",
    "            descriptions.append(desc)\n",
    "\n",
    "        # Increment the page count\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        page_count += 1\n",
    "\n",
    "    # Create a DataFrame from the collected descriptions, save to cache file\n",
    "    df = pd.DataFrame(descriptions, columns = ['Description'])\n",
    "\n",
    "    if cache_file:\n",
    "        df.to_csv(cache_file, index = False)\n",
    "\n",
    "    return df\n",
    "\n",
    "A_cache_file = directory + 'A_cache.csv'\n",
    "B_cache_file = directory + 'B_cache.csv'\n",
    "\n",
    "A_df = multiple_page_search(api_key, 'Yamaha Guitar Review', cache_file = A_cache_file)\n",
    "B_df = multiple_page_search(api_key, 'Gibson Guitar Review', cache_file = B_cache_file)\n",
    "\n",
    "merged = pd.DataFrame({\n",
    "    'Yamaha': A_df['Description'],\n",
    "    'Gibson': B_df['Description']\n",
    "})\n",
    "\n",
    "#print(merged['Yamaha'][2])\n",
    "#wv = api.load(\"word2vec-google-news-300\")\n",
    "#vec_king = wv['king']\n",
    "#print(vec_king)\n",
    "\n",
    "yamaha = \" \".join(merged['Yamaha'])\n",
    "gibson = \" \".join(merged['Gibson'])\n",
    "\n",
    "print(yamaha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
