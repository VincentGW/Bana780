{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Learn+to+Speak+Spanish&part=snippet&type=video&maxResults=50&key=AIzaSyDxZmWMX08dSD7s4H2CP9bfMwW53lzDjRY&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 90\u001b[0m\n\u001b[0;32m     87\u001b[0m B_cache_file \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB_cache.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m A_df \u001b[38;5;241m=\u001b[39m multiple_page_search(api_key, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLearn to Speak Mandarin\u001b[39m\u001b[38;5;124m'\u001b[39m, cache_file \u001b[38;5;241m=\u001b[39m A_cache_file)\n\u001b[1;32m---> 90\u001b[0m B_df \u001b[38;5;241m=\u001b[39m \u001b[43mmultiple_page_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLearn to Speak Spanish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mB_cache_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMandarin\u001b[39m\u001b[38;5;124m'\u001b[39m: A_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpanish\u001b[39m\u001b[38;5;124m'\u001b[39m: B_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     95\u001b[0m })\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpanish\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[33], line 59\u001b[0m, in \u001b[0;36mmultiple_page_search\u001b[1;34m(api_key, query, max_results, max_pages, cache_file)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m page_count \u001b[38;5;241m<\u001b[39m max_pages:\n\u001b[0;32m     52\u001b[0m     search \u001b[38;5;241m=\u001b[39m youtube\u001b[38;5;241m.\u001b[39msearch()\u001b[38;5;241m.\u001b[39mlist(\n\u001b[0;32m     53\u001b[0m         q \u001b[38;5;241m=\u001b[39m query,\n\u001b[0;32m     54\u001b[0m         part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m         pageToken \u001b[38;5;241m=\u001b[39m next_page_token\n\u001b[0;32m     58\u001b[0m     )\n\u001b[1;32m---> 59\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Collect video IDs from the response to retrieve full descriptions\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     video_ids \u001b[38;5;241m=\u001b[39m [item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Learn+to+Speak+Spanish&part=snippet&type=video&maxResults=50&key=AIzaSyDxZmWMX08dSD7s4H2CP9bfMwW53lzDjRY&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this problem, you will capture data that includes unstructured text and you will serve as a consultant analyzing this unstructured data using Natural Language Processing tools.\n",
    "There are many choices as to how you may do this. \n",
    "\n",
    "Minimum specifications:\n",
    "Identify a company to serve in this assignment. Their data should be part of the dataset you capture. Either...\n",
    "Find an API for download, or... \n",
    "\n",
    "A website where you can scrape a significant number of records (say 500 scraping, 1000 API) of text and other data about your company/product/service.\n",
    "Join at least two different datasets together (these could be multiple pages of web scraping or separate API requests for a product versus its competitor\n",
    "\n",
    "Create an analysis of unstructured data for your company/product and competitors using Python:\n",
    "\n",
    "Help us to understand what is being said about your company/product in the unstructured data (I will demo both API and screen scraping).\n",
    "Use natural language processing transformers to better access the MEANING of what the users are saying (ie. Word2Vec or Doc2Vec models).\n",
    "Create visuals and write explanations in a professional report that shows your findings.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "\n",
    "# Manage workspaces via Git\n",
    "current_directory = os.getcwd()\n",
    "directory = \"\"\n",
    "if current_directory[9:14] == 'vgwis':\n",
    "    directory = \"C:/Users/vgwis/Documents/Bana780/\"\n",
    "else:\n",
    "    directory = \"C:/Users/vgwcto/Documents/Python/Bana780/\"\n",
    "\n",
    "# Store API key locally\n",
    "api_key = pd.read_csv(directory + 'key.csv').columns[0]\n",
    "\n",
    "# Build a Function to easily search new ideas\n",
    "def multiple_page_search(api_key, query, max_results = 50, max_pages = 10, cache_file = None):\n",
    "    \n",
    "    # Check if cache exists\n",
    "    if cache_file and os.path.exists(cache_file):\n",
    "        return pd.read_csv(cache_file)\n",
    "\n",
    "    # Initialize the YouTube API client, variables\n",
    "    youtube = build('youtube', 'v3', developerKey = api_key)\n",
    "    descriptions = []\n",
    "    next_page_token = None\n",
    "    page_count = 0\n",
    "\n",
    "    # cONSTRUCT the API request\n",
    "    while page_count < max_pages:\n",
    "        search = youtube.search().list(\n",
    "            q = query,\n",
    "            part = 'snippet',\n",
    "            type = 'video',\n",
    "            maxResults = max_results,\n",
    "            pageToken = next_page_token\n",
    "        )\n",
    "        response = search.execute()\n",
    "\n",
    "        # Collect video IDs from the response to retrieve full descriptions\n",
    "        video_ids = [item['id']['videoId'] for item in response['items']]\n",
    "\n",
    "        video_request = youtube.videos().list(\n",
    "            part = 'snippet',\n",
    "            id = ','.join(video_ids)\n",
    "        )\n",
    "        video_response = video_request.execute()\n",
    "\n",
    "        for item in video_response['items']:\n",
    "            desc = item['snippet']['description']\n",
    "            descriptions.append(desc)\n",
    "\n",
    "        # Increment the page count\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        page_count += 1\n",
    "\n",
    "    # Create a DataFrame from the collected descriptions, save to cache file\n",
    "    df = pd.DataFrame(descriptions, columns = ['Description'])\n",
    "\n",
    "    if cache_file:\n",
    "        df.to_csv(cache_file, index = False)\n",
    "\n",
    "    return df\n",
    "\n",
    "A_cache_file = directory + 'A_cache.csv'\n",
    "B_cache_file = directory + 'B_cache.csv'\n",
    "\n",
    "A_df = multiple_page_search(api_key, 'Learn to Speak Mandarin', cache_file = A_cache_file)\n",
    "B_df = multiple_page_search(api_key, 'Learn to Speak Spanish', cache_file = B_cache_file)\n",
    "\n",
    "merged = pd.DataFrame({\n",
    "    'Mandarin': A_df['Description'],\n",
    "    'Spanish': B_df['Description']\n",
    "})\n",
    "\n",
    "print(merged['Spanish'][1])\n",
    "\n",
    "#test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
